\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage{xcolor}
\usepackage{amsmath, amsfonts, mathrsfs}

\title{\bfseries
A letter to \textsl{Gaia} DPAC CU5 regarding XP spectral extraction%
\footnote{This document was written after a meeting with Francesca di~Angeli, Dafydd Wyn Evans, Giorgia Busso, and Marco Riello (all \textsl{Gaia} CU5) in Cambridge, UK on 2023 May 19. That meeting itself was part of the 2023 \textsl{Gaia XPloration} meeting hosted by the Institute of Astronomy, University of Cambridge.}}
\author{David W. Hogg%
\footnote{Affiliations: Center for Cosmology and Particle Physics, Department of Physics, New York University; Max-Planck-Institut f\"ur Astronomie, Heidelberg; Flatiron Institute, a division of the Simons Foundation. Also: Member of \textsl{Gaia} DPAC CU9.}}
\date{May 2023}

% typesetting
\renewcommand{\paragraph}[1]{\medskip\par\noindent\textbf{#1}~---}
\addtolength{\textheight}{5.00cm}
\addtolength{\topmargin}{-1.00cm}
\addtolength{\textwidth}{-2.00cm}
\addtolength{\oddsidemargin}{1.00cm}
\linespread{1.07}
\pagestyle{myheadings}
\markboth{}{\color{gray}\sffamily Hogg / A letter re XP spectral extraction}
\sloppy\sloppypar\raggedbottom\frenchspacing

% mathematics
\newcommand{\dd}{\mathrm{d}}

\begin{document}

\maketitle\thispagestyle{empty}

\begin{abstract}\noindent
    The purpose of this document is to rephrase the \textsl{Gaia} XP spectral self-calibration and extraction in new language, and make recommendations for the sharp bits.
\end{abstract}

\section{Problem setup in my language}

We have many 60-pixel windows $n$, in each of which we have measured a 60-vector $y_n$ of counts or count rates.
These 60-vectors $y_n$ constitute \emph{the data}.
For now we assume that each of the windows $n$ contains significant amounts of intensity from one and only one star $m$.
We will need to think below also about the set (or maybe ordered list) $\mathscr{S}_m$, which is the set (or list) of all windows $n$ that contain star $m$ (and no other star).

Each window $n$ has associated with it three pieces of meta-data or housekeeping data:
The first is the identity $m$ of the star it contains.
The second is a blob $\alpha_n$ of information about the details of where and when that window was read, including (at least) the CCD row number, the pixel column of that CCD row, which field of view (FOV) is in play, focus information (for that FOV), and information about any across-scan source motion that might affect dispersion or resolution.
The third is a timing offset $\delta_n$, which gives (something like) the time difference between the reading of the fiducial corner of the window and the fiducial transit of the star.
In terms of form, $m$ is an integer or database index, $\alpha_n$ is an unstructured blob of information, and $\delta_n$ is just one floating-point number, measured in (something like) seconds or pixels.

The true (latent, unobserved) spectrum of this star $m$ in this window $n$ can be represented by a 55-vector $b_m$ of coefficients in one particular, God-given basis.
We don't care much about the details of this basis, but you can think of the basis as being built of 55 linearly independent, continuous functions of a continuous coordinate $x$, which we will call the pseudo-wavelength.

Before we start, here's an observation or assumption (restated effectively below):
Imagine we had a very special, very perfect focal plane, such that the spectral dispersion (wavelength solution) and resolution do not depend in any way on any of the housekeeping data in the $\alpha_n$ blobs.
\emph{This is not true!} We are just assuming it momentarily to make one point:
In this magical case, every observed 60-vector datum $y_n$ could be written accurately as
\begin{align}
    y_n &= B(\delta_n)\cdot b_m + \mbox{noise} ~,\label{eq:B}
\end{align}
where $B(\delta_n)$ is a $60\times 55$ design matrix created by evaluating the 55 God-given spectral basis functions at the 60 pixel locations, adjusting (correctly, of course!) for the timing offset $\delta_n$,
the product is a standard matrix product,
the vectors $y_n$ and $b_m$ are column vectors ($60\times 1$ and $55\times 1$ matrices, respectively),
and the term ``$+\mbox{noise}$'' is intended to indicate that there are small offsets created by photon noise and other variabilities, but no systematic biases (in this magical, hypothetical world).
That is, the spectral coefficients $b_m$ parameterize the appearance of the spectrum directly in the data, not some kind of calibrated or renormalized or deconvolved spectrum.
Another way to say it is that \emph{the spectra we extract here will be attenuated, pixel-convolved, LSF-convolved effective spectra, extracted at the fundamental resolution and throughput of the device}.

\section{Assumptions}

\paragraph{No backgrounds or detector issues}
We will assume that everything ``upstream'' in the \textsl{Gaia} pipelines is exactly correct and reliable, such that the data 60-vectors $y_n$ are not contaminated in any way by backgrounds, foregrounds, charge transfer issues, bias changes, or anything else that might violate the purely linear data model expressed in \eqref{eq:B}.
It is straightforward to modify the model to account for small residual background issues; we will return to this below.

\paragraph{No blends}
We will assume that every window $n$ has significant intensity contributions from exactly one star, and one star only, and that star is known.
This must be wrong in detail; it is possible but non-trivial to modify the model to account for blends and overlapping stars.

\paragraph{Non-enormous variations in dispersion}
Interestingly, the model (below) is so flexible, it will correct for (or extract internally self-consistent spectra in the presence of) variations in the spectral dispersion and resolution, using the internal self-consistency to learn the changes.
This will only be possible or stable in practice if the variations in the across-scan direction is not absurdly large.
This assumption is safe for \textsl{Gaia}.

\paragraph{Complete housekeeping data}
We assume that everything we need to know about the spacecraft state, including focus and rotation changes, is encoded in the housekeeping data blobs $\alpha_n$.
That is, the housekeeping data $\alpha_n$ are sufficient to accurately predict the particular dispersion and LSF relevant to window $n$, for every window $n$.

\paragraph{Low-resolution, uncalibrated outputs}
This assumption is implicitly stated above in \eqref{eq:B}:
The extracted spectra are extracted as they appear in the data, with no spectral or pixel deconvolution, and no correction for instrumental sensitivity as a function of wavelength.
This is what makes the extracted XP spectra so much fun.

\paragraph{Appropriate basis}
We will assume that the 55 God-given basis functions are chosen reasonably, have good support, and lead to stable numerical linear algebra.
I'm not convinced that this assumption holds, especially for stars of unusual colors.

\paragraph{Linearity}
The expected count rate in the detector, in every pixel, is a linear function of the true spectrum of the star.
This assumption is strong!
But since pixel shifts and convolutions are linear operations, it leaves scope for almost everything we think is going on.
Residual unsubtracted backgrounds or blends would violate this assumption, as would detector saturation or nonlinearity, and probably charge-transfer inefficiency.

\paragraph{Gaussian noise with known variance}
While it is not strictly necessary to have a good noise model---what happens below does not depend extremely strongly on the noise model---we will assume that the noise in the spectral pixels is Gaussian with approximately known variance in each pixel.
This assumption will be wrong in detail for many reasons, including cosmic rays, residual sky, and overlapping but unidentified fainter sources.
This assumption also becomes very wrong when the mean spectrum is being extracted for a highly variable star.

\section{Problems to solve}

The sensitivity, dispersion, and LSF all vary as a function of focal-plane (across-scan) position, and also as a function of FOV, focus settings, and spacecraft angular velocity.
By assumption, all of this information is in the housekeeping blob $\alpha_n$ for each window $n$.
The fundamental model for the XP spectral data---for both me and for the CU5 team---is (in my language)
\begin{align}
    y_n &= A(\alpha_n)\cdot B(\delta_n)\cdot b_m + \mbox{noise} ~,\label{eq:A}
\end{align}
where the projection operator $A(\alpha_n)$ is a $60\times 60$ matrix that is always and everywhere somehow ``close to'' the identity matrix, and $B(\delta_n)$ is the same $60\times 55$ design matrix that appears above in \eqref{eq:B}.
The projection operator function $A(\alpha)$ is close to the identity matrix, but it is also somehow ``smoothly varying'' with respect to sensible housekeeping parameters; in other words, windows nearby in the space of housekeeping data $\alpha_n$ should have similar projection operators $A(\alpha_n)$.

In the context of this model, as I see it, there are three high-level problems to solve here, which are all very closely related:

\paragraph{Calibrate}
Learn the projectors or convolution operators that project the stellar spectra into the pixels, as a function of the housekeeping data blob $\alpha$; that is, as a function of detector position, focus, FOV, and so on.
That is, learn an estimate $\hat{A}(\alpha)$ of the matrix-valued function $A(\alpha)$.

\paragraph{Extract mean spectra}
Deliver an estimate $\hat{b}_m$ of the spectral coefficient 55-vector representing the mean spectrum for (every) star $m$, by fitting to all the windows $m\in\mathscr{S}_m$. Also deliver some kind of uncertainty estimate on this.

\paragraph{Extract epoch spectra}
Deliver (in some useable form) an estimate of every individual visit spectrum $\hat{b}_n$, one per telemetered window $n$ (that meets criteria). Also deliver some kind of uncertainty estimate.

\medskip

We ought to solve all three of these problems (in my very strongly held opinion) by the optimization of a justifiable likelihood function, or (perhaps) a lightly regularized objective that is very close to a likelihood function.
The ``$+\mbox{noise}$'' in \eqref{eq:A} effectively implies a simple Gaussian likelihood function, which is
\begin{align}
    \ln\mathscr{L} &= \sum_n \ln\mathscr{L}_n \label{eq:lf1}\\
    \ln\mathscr{L}_n &= -\frac{1}{2}\,\chi^2_n - \frac{1}{2}\,\ln\det(2\pi\,C_n)\\
    \chi^2_n &= \Delta_n^\top\cdot C_n^{-1}\cdot \Delta_n \\
    \Delta_n &= y_n - A(\alpha_n)\cdot B(\delta_n)\cdot b_m ~,\label{eq:lf2}
\end{align}
where we have defined a per-window chi-squared value $\chi^2_n$,
we have defined a single-window 60-vector residual $\Delta_n$,
and $C_n$ is a $60\times 60$ noise covariance tensor for window $n$, perhaps quite accurately diagonal.

Related to the three high-level problems stated above, I see many inter-related technical issues that are likely to arise:

\paragraph{Choice or control of the fiducial system}
The extracted spectra are supposed to look like the spectra in the data; that is, they are supposed to be at XP resolution and dispersion, with a shape that represents the system throughput.
Since throughput, dispersion, and resolution vary with across-scan location (and FOV and focus and so on), the question arises:
What is the fiducial throughput, resolution, and dispersion at which the spectra are extracted?
This question boils down to:
Where (if anywhere) in the device or dataset is the projection operator $A(\alpha)$ equal to the $60\times 60$ identity matrix $\mathbb{I}$?
It could be that there is a fiducial setting of the housekeeping data $\alpha_\ast$ such that $A(\alpha_\ast)=\mathbb{I}$, or it could be that some average or mean instrument has
\begin{align}
    \left[\int_\mathscr{D} \dd\alpha\right]^{-1}\,\displaystyle\int_\mathscr{D} A(\alpha)\,\dd\alpha  &= \mathbb{I} ~,\label{eq:meanA}
\end{align}
for some domain of integration $\mathscr{D}$.

\paragraph{Parameterization of the projection operator function}
The projection operator function $A(\alpha)$ is expected to vary smoothly with respect to continuous parameters such as across-scan position and telescope focus.
It is also likely that it varies smoothly with pseudo-wavelength $x$ in the sense that adjacent rows of the matrix returned by $A(\alpha)$ should look similar except for a shift left or right.
Smoothness of these kinds could be enforced by explicitly making the parameterization of the function smooth (with polynomials or splines, say), or by adding regularization to a more flexible (non-parametric, even) model.

\paragraph{Exact and near-exact degeneracies}
The expression \eqref{eq:A} contains a panoply of degeneracies, many of them exact.
The fact that the projection operator $A(\alpha)$ has (in principle) 3600 free parameters while the extracted spectrum has only 55 free parameters means that there will be an enormous degeneracy, even with strong constraints on how $A(\alpha)$ can vary with $\alpha$ and pixel number.
Concretely, for example, the $A(\alpha)$ can be scaled up by a factor, and $b$ down by a factor to keep the prediction for $y$ unchanged.
For another example, the projector can get sharper (scaled up in some parts of Fourier space) and the spectrum can get smoother (scaled down in those same parts of Fourier space), to keep the prediction for $y$ unchanged.
Naively one might expect that the choice of the fiducial system will break these degeneracies, since it sets the value of $A(\alpha_\ast)$, but that isn't necessarily true \emph{in practice} when most of the windows $n\in\mathscr{S}_m$ are not near the fiducial system.

\paragraph{Stability of inference}
When individual-visit spectrum estimates $\hat{b}_n$ are made, they will probably involve something like the optimization of a 55-parameter likelihood function constrained by only 60 data points in window $n$, or maybe the pseudo-inverse of a $60\times 55$ matrix that is something like the product $A(\alpha_n)\cdot B(\delta_n)$.
This operation is likely to be very ill-conditioned, especially for stars that are faint or very red (say).
The mean spectrum estimates $\hat{b}_m$ ought to be much better conditioned than the individual-epoch estimates $\hat{b}_n$, because there are many windows constraining each mean spectrum.
However, even among the mean-spectrum estimates there will likely be some badly behaved cases.

\section{Conjectures and suggestions}

\paragraph{Maximum likelihood with light regularization}
My primary (unsolicited) recommendation is that the estimates $\hat{A}(\alpha), \hat{b}_m, \hat{b}_n$ all be maximum-likelihood estimates, found by optimizing the likelihood function laid out in \eqref{eq:lf1} through \eqref{eq:lf2}, constrained by the data from all windows $n$ (perhaps subject to quality cuts).
And, indeed, this is what I believe that CU5 is doing right now.

Alternatively, the estimates $\hat{A}(\alpha), \hat{b}_m, \hat{b}_n$ could be found by optimizing a lightly regularized version of the likelihood, but I would recommend that any regularization primarily be aimed at breaking degeneracies, and not strongly constraining the solution.
Finally, if there \emph{are} strong regularizations to be applied, those strong regularizations should be only on the properties of the projection operator function $A(\alpha)$, because this is the part of the problem that we least care about (from an astronomical-research perspective), and thus least distorts downstream research conclusions.

\paragraph{Cubics and other polynomials}
The XP pipeline currently involves a few (maybe many) cubic polynomials.
Polynomials have some bad properties for problems like this; I recommend replacing them with a different---but still linear---basis.
The fundamental issue is that polynomials are not translation-invariant in their properties; they go wild at the edges.
This both distorts the results at the edges of the domain, and also makes the data at the edges of the domain unfairly powerful in setting the polynomial coefficients.

The first recommended replacement for polynomials is sines and cosines.
The key idea here is that these are translationally invariant and easily computed.
The one detail is that, if you have a domain that goes from $0$ to $L$, you should use angular wave numbers $k$ that are integer multiples of $\pi/L$ not $2\pi/L$.
The reason is that if you use $2\pi/L$ you are assuming that your functions are periodic on your domain, which is not generally true.
If you use $\pi/L$ you maintain linear independence but make no such assumption.

The second recommended replacement is a cubic spline with chosen knots.
This is a beautiful and flexible solution!
However, the construction of design matrices analogous to those used in polynomial fitting requires that you have a differentiated spline code, and these are somewhat hard to come by.
So I usually prefer sines and cosines to splines with knots.

\paragraph{How to parameterize the projection operator function}
The projection operator function $A(\alpha)$ in principle has a huge number of degrees of freedom; how to limit them?
Right now I believe that CU5 makes the projection matrix 5-diagonal, with the matrix values restricted to be cubic functions of across-scan position within each CCD row.
I believe that they also require that the matrix entries be smooth with respect to the matrix row number (or window along-scan pixel number).
Instead of this, I would consider making some kind of expansion, such that the matrix $A(\alpha)$ can be written in terms of a sum of (small, we hope) adjustments away from the $60\times 60$ identity matrix $\mathbb{I}$.
This would look like
\begin{align}
    A(\alpha) = \mathbb{I} + \sum_{j=1}^J a_j(\alpha)\,A_j~,
\end{align}
where the (small) amplitudes $a_j(\alpha)$ vary (smoothly if you like) with the housekeeping data $\alpha$ and the $A_j$ are a set of linearly independent $60\times 60$ matrices.

I love this idea!
However, I have to admit that I have absolutely no idea how to set the $A_j$ matrices.
One option would be to learn them simultaneously with the amplitudes $a_j$ and also the spectra.
Another option would be to make a God-given basis of $A_j$ matrices (analogous to the $B(\delta)$ basis), where that God-given basis is designed to capture the observed variation in the $A(\alpha_n)$ matrices that have been inferred from the \textsl{Gaia} DR3 XP data extraction.

My intuition is that this kind of expansion for the projection operators is a much better way to control the complexity than hard-setting the matrices to be 5-diagonal.
I have a further (weaker) intuition that if this expansion was built well, smoothness might not need to be enforced as strongly in the parameterization of the amplitudes $a_j(\alpha)$ as it is currently in the parameterization of the 5-diagonal setup.

\paragraph{Controlling the fiducial system}
My current understanding is that the CU5 system intends for the fiducial system to be somehow the ``mean'' system.
But I have a feeling that this is a hope for the code, and not an explicit requirement put on the code.
If you want to require this, you could add a regularization penalty or explicit hard constraint on the optimization that enforces that the average (mean or median even) of $A(\alpha_n)$ over some set of windows $n$ (maybe all of them) is exactly the $60\times 60$ identity matrix $\mathbb{I}$.
Alternatively the system could constrain a constraint or regularization that forces $A(\alpha_\ast)=\mathbb{I}$ for some particular window location and spacecraft state $\alpha_\ast$.
I have two intuitions here.
The first is that it is important to be explicit about where and how the fiducial system is defined and force the optimization to respect that choice.
The second is that it will be more robust to set the fiducial system with some kind of average than by setting it to a fiducial point.
The average could be a made-up mean integral like \eqref{eq:meanA} or it could be the empirical average over the actual windows.
The reason that I think the mean system will be more robust is that it gets contributions from many windows, and not just windows in a rare slice of the focal plane.

\paragraph{Degeneracies}
As noted above, there are many exact and near-exact degeneracies in this problem.
The thing I'm confused about is: If you set the fiducial system explicitly, do you break all of them?
I somehow feel like you don't!
But then I can't find any simple degeneracies that survive (at least not by pure thought).

\paragraph{Are there residual background issues?}
If there are small unmodeled background contributions to some windows, it probably wouldn't be expensive or biased to just straight-up estimate them simultaneously with the spectral vectors $\hat{b}_m$.
This would look like adding a constant term to \eqref{eq:lf2} to make it
\begin{align}
    \Delta_n &= y_n - A(\alpha_n)\cdot B(\delta_n)\cdot b_m - s_n~,
\end{align}
where $s_n$ is a ``sky level'' for window $n$.
In this model, there is one mean spectrum $b_m$ for all windows $n\in\mathscr{S}_m$, but there is a separate sky $s_n$ for each of those windows.
Because the constant sky is such a rigid model, and because the basis functions drop near to zero at the window edges, the new inference for the $\hat{b}_m$ found while also simultaneously estimating all the $s_n$ values would still be very well behaved.
It might require a small restructuring of the code, because the star $m$ would now have not 55 parameters, but instead $55+N_m$, where $N_m$ is the size of the set $\mathscr{S}_m$.

\paragraph{Stably estimating the individual epoch spectra}
Let's start with some context: The mean spectrum estimate $\hat{b}_m$ for star $m$ is found by maximizing the likelihood---the probability of the data---with respect to the stellar spectrum $b_m$.
This will look something like
\begin{align}
  \hat{b}_m &\leftarrow [X_m^\top\cdot C_m^{-1}\cdot X_m]^{-1}\cdot X_m^\top\cdot C_m^{-1}\cdot Y_m ~,
\end{align}
where $X_m$ is a big $60\,N_m\times 55$ design matrix made by stacking up all the products $A(\alpha_n)\cdot B(\delta_n)$ for all $n\in\mathscr{S}_m$,
$N_m$ is the number of relevant windows or the size of the set $\mathscr{S}_m$,
$C_m$ is a big diagonal $60\,N_m\times 60\,N_m$ covariance matrix made by clicking together all the individually diagonal covariance matrices $C_n$,
and $Y_m$ is a big $60\,N_m\times 1$ column vector made by stacking all the data vectors $y_n$.

Note that the estimate $\hat{b}_m$ should be made without ever explicitly constructing $C_m$, because that covariance matrix is almost entirely zeros.
It should also be made without ever actually running the linear algebra \texttt{inv()} function.
It is way better to use the function \texttt{solve()} or even better \texttt{lstsq()}, where in the latter case you can set the condition number to eliminate very-near-zero eigenvalues.

If you want to estimate the individual epoch spectra $\hat{b}_n$---but see below where \emph{I recommend that you never estimate these}---the inference looks similar; it is
\begin{align}
  \hat{b}_n &\leftarrow [X_n^\top\cdot C_n^{-1}\cdot X_n]^{-1}\cdot X_n^\top\cdot C_n^{-1}\cdot Y_n\\
  X_n &= A(\alpha_n)\cdot B(\delta_n)~.
\end{align}
In principle everything is just the same.
The only issue is that this inference will be very ill-conditioned; some eigenvalues of the matrix to be inverted will be very close to zero.
One option is to regularize this inversion, which is equivalent to adding some kind of prior in the spectral space.
Another is to explicitly fix low eigenvalues, by using the \texttt{lstsq()} instead of the \texttt{solve()} operator.
This also is equivalent to adding some kind of prior, but without knowing what that prior is!
I don't love either option here, because stability of inference will come at the cost of bias, and astronomers hate bias.
All this motivates my final, radical suggestion, below.

\paragraph{Don't estimate the epoch spectra at all}
Here's my most radical suggestion: Instead of providing an estimate $\hat{b}_n$ for every window $n$---each of which would naturally be produced by an inference involving a poorly conditioned matrix pseudo-inverse or \texttt{lstsq()} operation---provide instead the \emph{inputs to that inference}.
That is, release the epoch spectra by releasing the raw data 60-vectors $y_n$ of counts and also release an interface to generate the projection operators $\hat{A}(\alpha_n)$ appropriate for each of those raw data vectors.

Is this dereliction of duty? No! Provided that the project provides a usable interface for performing inferences or operations with the epoch spectra.
For example: Imagine that a star flares. This flare can be found by comparing each epoch datum $y_n$ for $n\in\mathscr{S}_m$ to the synthesis from the mean spectrum estimate $\hat{b}_m$:
\begin{align}
    \hat{\Delta}_n = y_n - \hat{A}(\alpha_n)\cdot B(\delta_n)\cdot \hat{b}_m ~,
\end{align}
where (in my imagination), the operators $\hat{A}(\alpha_n)$ and $B(\delta_n)$ will be provided by a Gaia-hosted or Gaia-built interface similar to \texttt{GaiaXPy},
and the $\hat{\Delta}_n$ gets a hat because it is an estimate of the residual at the epoch.
For another: Imagine that there is a periodic variation with a period $P=2\pi/\omega$.
The (complex) varying spectral component $b'_m$ can be estimated with something like the likelihood function \eqref{eq:lf1} through \eqref{eq:lf2} but with \eqref{eq:lf2} modified to
\begin{align}
    \Delta_n &= y_n - \hat{A}(\alpha_n)\cdot B(\delta_n)\cdot [b_m + b'_m\,\exp{i\,\omega\,t_n}] ~,
\end{align}
where we have used the barycentric time $t_n$ for each window $n$ explicitly here and taken the real part implicitly.
Once again, we assume that the operators $\hat{A}(\alpha_n)$ and $B(\delta_n)$ will be provided by a Gaia-hosted or Gaia-built interface.
My point is that even if the individual-epoch $\hat{b}_n$ estimation is unstable, the estimation of flares or periodic spectral variations $b'_m$ will not be unstable.
\emph{It is better to provide the tools to do stable inferences than it is to publish the results of unstable inferences.}

If the project switched from the goal of producing spectral estimates $\hat{b}_n$ for each window $n$ to releasing the raw data $y_n$ and the operators $A(\alpha_n)$ and $B(\delta_n)$, then it would make sense to build some tutorials and documentation to demonstrate to the community how to use these data products.
I would be very willing to help make, write, and test such tutorials and documentation.

\end{document}
